# Overview
In the context of scientific peer review, Glimpse is a summarization method designed to generate concise yet comprehensive overviews of scholarly reviews. It introduces novel uniqueness scores based on the Rational Speech Act framework to identify key sentences in reviews.

This repository reproduces the results from the original Glimpse paper and extends its application to a new domainâ€”book reviews on Amazon. Additionally, I modified the original scoring mechanism by replacing the PEGASUS language model with FLAN-T5 (Base & Large) to evaluate performance differences.


## Dataset

### Scientific Reviews (Original Dataset)
- The original dataset consists of ICLR conference reviews collected from OpenReview.
- It includes 28,062 reviews from 8,428 submissions spanning 2017-2021.
- Each paper has a unique ID and a meta-review, which serves as the gold standard for evaluation.

### Amazon Book Reviews (Extended Dataset)
- The extended dataset consists of Amazon user reviews for books.
- To ensure compatibility with the original model, the dataset was structured similarly:
  - Each book's ID appears multiple times, corresponding to different reviews.
  - A separate file aggregates all reviews for each book.
  - GPT-4o was used to generate "gold standard" summaries for evaluation.
- The original dataset is available here: [Kaggle - Amazon Books Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews?select=Books_rating.csv)


## Original Model
Glimpse operates in two settings:
- **Extractive Summarization:** Candidate summaries are generated by extracting sentences from reviews.
- **Abstractive Summarization:** PEGASUS or BART are used to generate summaries.

## Proposed Extensions
This project introduces two key extensions:
- **Domain Adaptation:** Applying the model to book reviews instead of scientific reviews.
- **Evaluation Model Switch:** Changing the scoring model from PEGASUS to FLAN-T5 to analyze performance differences.
